{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs fullkv on GSM 8K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import sglang as sgl\n",
    "\n",
    "os.makedirs(\"./z_experiment/evaluation/\", exist_ok=True)\n",
    "os.makedirs(\"./z_experiment/output/\", exist_ok=True)\n",
    "os.makedirs(\"./z_experiment/results/\", exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "    save_path: str = \"./z_experiment/output/test_output.jsonl\"\n",
    "    data_size: int = 10\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "PROMPT_TEMP = \"You are given a math problem.\\n\\nProblem: {question}\\n\\n You need to solve the problem step by step. First, you need to provide the chain-of-thought, then provide the final answer.\\n\\n Provide the final answer in the format: Final answer:  \\\\boxed{{}}\"\n",
    "\n",
    "test_data = []\n",
    "prompts = []\n",
    "with open(\"./evaluation/data/test_one.jsonl\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == config.data_size:\n",
    "            break\n",
    "        sample = json.loads(line)\n",
    "        prompt = PROMPT_TEMP.format(question=sample[\"question\"])\n",
    "        prompts.append(prompt)\n",
    "\n",
    "        sample[\"prompt\"] = prompt\n",
    "        sample[\"index\"] = idx\n",
    "\n",
    "        test_data.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 03:30:51 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 03:30:51,523\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-03 03:30:53,191 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 03:30:57 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 04-03 03:30:57 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 03:30:59,605 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compress_algorithm = \"CustomKV\" # \"SnapKV\", \"CustomKV\"\n",
    "compress_max_window = 8\n",
    "compress_max_prompt = 128\n",
    "compress_divide_length = 64\n",
    "compress_divide_method = \"newline\" # \"step_length\", \"newline\"\n",
    "\n",
    "llm = sgl.Engine(\n",
    "    model_path=config.model_path,\n",
    "    dtype=\"bfloat16\",\n",
    "    disable_overlap_schedule=True,\n",
    "    compress_algorithm=compress_algorithm,\n",
    "    compress_max_window=compress_max_window,\n",
    "    compress_max_prompt=compress_max_prompt,\n",
    "    compress_divide_length=compress_divide_length,\n",
    "    compress_divide_method=compress_divide_method,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    sampling_params = {\"temperature\": 0.0, \"top_p\": 0.95, \"max_new_tokens\": 8192}\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    end_time = time.time()\n",
    "\n",
    "    llm.shutdown()\n",
    "\n",
    "    for sample_idx, output in enumerate(outputs):\n",
    "        test_data[sample_idx][\"output\"] = output[\"text\"]\n",
    "        test_data[sample_idx][\"prefill_tokens\"] = output[\"meta_info\"][\"prompt_tokens\"]\n",
    "        test_data[sample_idx][\"output_tokens\"] = output[\"meta_info\"][\n",
    "            \"completion_tokens\"\n",
    "        ]\n",
    "        test_data[sample_idx][\"total_tokens\"] = (\n",
    "            output[\"meta_info\"][\"prompt_tokens\"]\n",
    "            + output[\"meta_info\"][\"completion_tokens\"]\n",
    "        )\n",
    "\n",
    "    with open(config.save_path, \"w\") as fp:\n",
    "        for line in test_data:\n",
    "            fp.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    total_tokens_generated = sum(\n",
    "        output[\"meta_info\"][\"completion_tokens\"] for output in outputs\n",
    "    )\n",
    "    throughput_tokens = total_tokens_generated / total_time\n",
    "    throughput_requests = len(prompts) / total_time\n",
    "\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Throughput (tokens/s): {throughput_tokens:.2f}\")\n",
    "    print(f\"Throughput (requests/s): {throughput_requests:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 03:31:08,609 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-04-03 03:31:08,642 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-04-03 03:31:09,548 - INFO - flashinfer.jit: Loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n",
      "2025-04-03 03:31:09,581 - INFO - flashinfer.jit: Finished loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m prompts = prompts[:\u001b[32m100\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     20\u001b[39m sampling_params = {\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.95\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m8192\u001b[39m}\n\u001b[32m     22\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m end_time = time.time()\n\u001b[32m     26\u001b[39m llm.shutdown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/kewan/sglang-CustomKV/python/sglang/srt/entrypoints/engine.py:164\u001b[39m, in \u001b[36mEngine.generate\u001b[39m\u001b[34m(self, prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, lora_path, custom_logit_processor, stream)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generator_wrapper()\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     ret = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__anext__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/kewan/.conda/envs/sglang-dev/lib/python3.12/selectors.py:468\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    466\u001b[39m ready = []\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "prompts = prompts[:100]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Use the following commands to evaluate GSM8k results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python evaluation/math_eval_all_v2.py \\\n",
    "    --exp_name \"evaluation\" \\\n",
    "    --output_dir \"./z_experiment/results\" \\\n",
    "    --base_dir \"./z_experiment/output\" \\\n",
    "    --dataset gsm8k\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
